%\documentclass[sigconf, anonymous]{acmart}
\documentclass[sigconf]{acmart}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{booktabs} % For formal tables
\usepackage{footnote}
\usepackage[lofdepth,lotdepth]{subfig}
\usepackage[ruled]{algorithm2e} % For algorithms
\usepackage{listings, xcolor}
\usepackage{todonotes}\setlength{\marginparwidth}{3.5cm}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}
\acmDOI{}

% ISBN
%\acmISBN{123-4567-24-567/08/06}
\acmISBN{}

%Conference
\acmConference[CGO'18]{International Symposium on Code Generation and Optimization}{February 2018}{Vienna, Austria} 
\acmYear{2018}
%\copyrightyear{2016}

%\acmPrice{15.00}


\begin{document}
\title{SPERF a scalability profiler for shared-memory parallel applications}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}

\author{Vitor~R.~G.~Silva}
%\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Univ. Federal do Rio Grande do Norte}
  \streetaddress{Departamento de Engenharia de Computação e Automação}
%  \city{Natal - RN} 
%  \state{RN} 
%  \postcode{59078-900}
}
\email{ramos.vitor89@gmail.com}

\author{Marcio~O.~Jales}
%\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Univ. Federal do Rio Grande do Norte}
  \streetaddress{Departamento de Engenharia de Computação e Automação}
%  \city{Natal - RN} 
%  \state{RN} 
%  \postcode{59078-900}
}
\email{marcio.dojcosta@gmail.com}

\author{Alex~F.~A.~Furtunato}
\affiliation{%
  \institution{IFRN}
  \streetaddress{Diretoria Acad\^{e}mica de Gest\~{a}o e Tecnologia da Informa\c{c}\~{a}o}
%  \city{Natal - RN} 
%  \state{RN} 
%  \postcode{59078-900}
}
\email{alex.furtunato@ifrn.edu.br}

\author{Luis~F.Q~Silveira}
\affiliation{%
  \institution{Univ. Federal do Rio Grande do Norte}
  \streetaddress{Departamento de Engenharia de Computação e Automação}
%  \city{Natal - RN} 
%  \state{RN} 
%  \postcode{59078-900}
}
\email{lfelipeqs@gmail.com}

\author{Samuel~Xavier-de-Souza}
\affiliation{%
  \institution{Univ. Federal do Rio Grande do Norte}
  \streetaddress{Departamento de Engenharia de Computação e Automação}
%  \city{Natal - RN} 
%  \state{RN} 
%  \postcode{59078-900}
}
\email{samuel@dca.ufrn.br}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Vitor R. G. Silva et al.}

\begin{abstract}
SPERF is presented as a tool for performing automating measurements of efficiency for scalability analysis of parallel applications in shared memory systems. With a simple configuration timings measurements are taken when varying the number of threads and input sizes for a target application. The measurements are taken at relevant points of the code, such as loops and parallel region delimiters. This allows a hierarchical scalability analysis to easily identify specific regions of code that, when optimized, are the best candidates to improve the overall scalability of the program. The tool is written in C++ and supports the analysis of C\/C++ programs that have been parallelized with OpenMP or PThreads. A statistical analysis of the tool shows that it has a minimal level of intrusion since the target application's performance measured using the tool is basically the same as if it was measured without using it.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10010940.10011003.10011002</concept_id>
<concept_desc>Software and its engineering~Software performance</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software performance}

\keywords{Parallel Computing, Muticore Architecture, Parsec Benchmark}

\maketitle

\section{Introduction}
\todo[inline,caption={REVISÃO: teste de nota}]{REVISÃO: linha 1.
\newline linha 2 ....}
Existing profiling tools focus on analyzing hotspots or bottlenecks using statistical measurements from a single run. Naturally, many applications may present different bottlenecks/hotspots for different input sizes. In the multi-core and many-core era, bottlenecks may also change when varying the number of cores used in the computation. Moreover, eliminating performance bottleneck from a single representative run is not sufficient to ensure that the program will scale its performance along with newer generations of multicore/manycore processors because scalability concepts involve analyzing the normalized performance across runs. 

The fundamentals metrics to measure the parallel execution are Speedup and Efficiency. The Speedup, $S$, is related with measures of execution time of task on parallel application and is defined how:

\begin{equation}
  S = \frac{T_{S}}{T_{P}}
  \label{eq:speedup}
\end{equation}
where, $T_{S}$ is the measure of task execution time on single core mode, and $T_{P}$ is the measure time of a task running on multi-core mode. The Efficiency, $E$, of a parallel application is defined by the ratio of Speedup, $S$, by  the number of cores, $N$, used to running that application, measured on partial or entire code. Thus, the Efficiency is:

\begin{equation}
  E = \frac{S}{N}
  \label{eq:efficiency}
\end{equation}

Collecting efficiency information is the first step on identifying bottlenecks on software scalability. In the traditional approach, collecting the speedup information demands the declaration of variables that will be used to time the application, use of appropriate functions to take the time information, handle the resulting information and calculation and execute the application over and over again, this process demand some work.

In this paper we present a tool for automate this process along with her configurations and features on section \ref{sec:sperf}. Also we validate our tool showing that the intrusiveness of this tool is minimal on section \ref{sec:validation}. To demonstrate analysis that can be done with Sperf we show on section \ref{sec:results} the results on some well known benchmarks applications. And, for the last, the Sections \ref{sec:future_work} and \ref{sec:conclusions} discuss about our conclusion and listed some wanted features that can be attended by future works.


\section{Scalability Measurement}
\label{sec:scalability_measurement}
Scalability on parallel system is the capability of handle growing on the problem size. There are three types that we can classify an program on:
\begin{itemize}
\item Scalable if by increasing the problem size with a fixed number of cores the efficiency not decreases.
\item Weak scalable the efficiency when the problem size increases with the same rate of the cores have to be constant or grow.
\item Strong scalable keeping the problem size fixed and increasing the number of cores the efficiency does not decreases. 
\end{itemize}
The scalability is an important measurement on a parallel system because the most commons ways of parallelization is by data division where each core gets a slice of the input data to process and on this systems the length of the date is very important.

\subsection{Profiling for scalability measure}
\label{subsec:profiling}

There are several ways of profiling the application, and they can be classified by how the collect data into two classes, event base or statistical.

\begin{itemize}
\item Event base works by interrupt the program each time a certain event occurs like enter o leaving a function, system calls and others.
\item Statistical profiling operating by sampling the call stack of the target program on regular intervals.
\end{itemize}

On the profile based on events that is triggered by system calls or function call can cause performance changes due to the high intrusiveness and can have some side effects such as on memory caches or instruction decoding pipeline being able to causing inaccurate results. Statistical in the other hand can be less intrusive and also have less side effects but is not numerical accurate.

On the HPC (High-performance computing) community numerous researches have proposed plenty of tools that aim to profile applications like \cite{Kufrin2005}, \cite{Servat2016}, \cite{Morris2010},
\cite{Graham1982}, \cite{Shende2006}, \cite{Malony2014}
\cite{Servat2013}, \cite{Servat2013}, \cite{Adhianto2010a} \cite{Chittimalli2012}, \cite{Tiku1971}, \cite{Knupfer2012}, \cite{Southern2016},
\cite{Woo1995}, \cite{Bienia2008}, \cite{Bienia2010} \cite{Adhianto2010}, \cite{Srivastava}, \cite{Tallent}
\cite{Architecture2015}, \cite{Tiku1971a}, \cite{Tallent}
\cite{Adhianto2010}, \cite{Srivastava}
frameworks Score-p \citep{Knupfer2012}, ATOM \cite{Srivastava}. Fine-grained profiles Scalable fine-grained call path tracing \cite{Tallent} 
Gprof \cite{Graham1982}, Call path profiler \cite{Adhianto2010} not sure Splash \cite{Woo1995} 

But these tools focus on profiling the target application with a fixed input and/or a fixed number of threads, in this way is not possible to see the overall scalability of the program. SPERF was created on this purpose running the target application multiple times with different inputs and number of threads. This way the developer can look at the scalability of each region on multiple core and data length.

%The HPCToolkit solution, for example, consists of a large project that provides a call path tracing with low-overhead and asynchronous statistical sampling to collect fine-grained context-sensitive data across multiple threads. To support tasks as performance tuning, runtime scheduling and automatic parallelization, a data-dependence profiler has been developed. Finally, a profiler tool suited to runtime managed environments and even to profile heterogeneous parallel code are among the solutions to collect sensitive metrics to support the parallel performance analysis. 


\section{SPERF}
\label{sec:sperf}

SPERF join the best of the ways of profiling two being less intrusive making possible to analyze the real time of the application with precision.Was developed in C programming language inside a Linux system environment and compiled with GNU\/g++ standard 98 with the focus on see the overall scalability and it is a event based profiling, triggered by specific functions to do this its performance a instrumentation on the source code that insert function on the begin and in the end of parallel region. And these functions are used to collect data.

The instrumentation is done by adding two marks on the region of interest. To do this there is two functions available on the SPERF package on the sperfops.h file, sperf\_start(n) and sperf\_stop(n). Those two functions annotates the region where the time will be acquired, sperf\_start delimits the beginning of the area, while sperf\_stop marks the end. These functions use the id passed by argument to get the time of each thread spent on the region marked.They may be used as many times as wished by the user, in order to time multiples regions. Therefore, the user should pass to these functions, as argument, an integer value that enumerates the correspondent parallel regions. In other words, if one wishes to time "n" areas of code, the first area should be delimited by sprof\_start(1) and sprof\_stop(1), the second by sprof\_start(2) and sprof\_stop(2) and so on.

This step of instrumentation can be done automatically on programs that use OPENMP \cite{Architecture2015} where the tool will seek for OPENMP directives and will mark the begin and end of the each parallel region, but manual regions can also be added.

After that the user can compile the program as was compiled before there is no special library needed and profile the application by invoke SPERF and pass, as command line arguments, the target application path (sperf app). Thus. SPERF will read a configuration file that tell the number of threads and all arguments that program will run, them execute the program from all possible combinations.To set the number of threads in the target application SPERF uses an environment variable OMP\_NUM\_THREADS on OPENMP application or pass by argument to the program where the user configure previously what argument in his application is the number of threads.Then on each call to sperf\_stop will be send information about the time spent on the parallel region using pipes method of interprocess communication on C\/Linux and SPERF will annotate the time of each region marked and output to the user a file with these informations.

\subsection{Options available}

The tool have the following options that is passed by command line:
\begin{itemize}
\item -i -p instrument use to performance the automatic instrumentation on the source code the next argument is the path to the source code where there is going to instrument all .cpp and .c files by default also is possible the user tell what extensions will be instrumented using -e option the next argument is the extensions separated by ','
\item -o output file set the name of the output file by default the name is the same of the application.
\item -c configuration set the name of the configuration file if no option is passed the program will look for the configuration file with the same name of the application. (use to configure testes that will be performed )
\item -t specify the type of output file, the can be JSON, XML and CSV.
\end{itemize}

\subsection{Execution configuration}
To configure how the profiler will run, SPERF have a configuration file. This file consist of 6 variables.

There is two ways of setting the threads that target will run, one is listing all desired values using the variable list\_threads\_values={1,2,3,6,8} and the other is by configuring automated steps that can be arithmetic progression (constant increase) and geometric progression.To do this there three variable type\_of\_step set the type of progression power or constant, value\_of\_step sets the increasing rate and max\_number\_threads set the maximum value of thread.

Furthermore the user can set a list of arguments with the variable list\_of\_args={arg11 arg12 ..., arg21 arg22 ..., ...}

\begin{itemize}
\item number\_of\_tests - number of times that SPERF will run.
\item list\_threads\_values - set specific values from number of threads.
\item type\_of\_step (power or constant) - set the type of increasing can be either constant rate of power.
\item value\_of\_step - set the value of the step.
\item max\_number\_threads - set maximum value of thread .
\item list\_of\_args - list of the argument separated by ",".
\end{itemize}

How many times the "test" will run, the initial and final number of threads that the target will use, how will be the increment from the initial value and the final value and how many tests will be performed. Regarding to the increment, there are two manners: arithmetic progression (constant increase) and geometric progression. That means, if an initial and final value of 1 and 16 are set, respectively, with constant increment 2, the target application will execute using 1, 3, 5, 7 and so on until 16 threads (15 will be the last in this case). Besides, the user informs how many times the target will be tested. So, considering the previous case, providing a value of 50, the cycle of 1 to 15 threads with constant increment of 2 will repeat 50 times.

\subsection{Data visualization}
The default output from SPERF is a JSON (JavaScript Object Notation) file from each parallel region marked. This archive contains information about arguments, threads, the begin and end line of the region on the code, speedup, efficiency and time from execution. But it can also produce files in XML(eXtensible Markup Language) and CVS (Comma-separated values).

The format on the JSON has the tags, "Region" informing the initial and final line of the parallel region, "Filename" with the name of the source file that the regions was included, "Execution" where has information about all input data used and inside each the tag input data with information of watch arguments was used and all the runs with different number of threads.

%TODO : Change the tags for English. Why XML? Why not JSON?
\lstset{
    string=[s]{"}{"},
    stringstyle=\color{red},
    comment=[l]{:},
    commentstyle=\color{black},
}

\begin{lstlisting}
{
"Region":"initial line, final line",
"Filename":"program.cpp",
"Execution 1":{
	"InputData 1":{
		"Arguments":"arg1 arg2",
		"Run 1":{
			"Number of threads":"1",
			"Time":"t",
			"Speedup":"s",
			"Efficiency":"e"},
		...
		"Run m":{
			...
		}
	},
	...
	"InputData n":{
		...
	}
},
...
"Execution e":{
	...
}
}
\end{lstlisting}

The CSV output file is a simple table that show the efficiency for the combination of each argument with all threads.The sample CSV file.

% Table
\begin{table}[H]%
\caption{Sample CSV}
\label{tab:sampleCSV}
\begin{minipage}{\columnwidth}
\begin{center}
\begin{tabular}{lllll}
     & num\_threads & num\_threads & num\_threads & num\_threads \\
arg1 & efficiency & efficiency & efficiency & efficiency \\
arg2 & efficiency & efficiency & efficiency & efficiency \\
arg3 & efficiency & efficiency & efficiency & efficiency \\
arg4 & efficiency & efficiency & efficiency & efficiency \\
\end{tabular}
\end{center}
\bigskip\centering
\footnotesize\emph{Source:} Author
\emph{Note:} Efficiency for the entire application.
\end{minipage}
\end{table}%

\section{Validation}
\label{sec:validation}

The validation of our tool is the next step of the development process, as fundamental as the tool coding process itself. The tests were performed using two target applications, called, Blackscholes and Freqmine, executed on a server structured with 2 CPUs Intel Xeon Sixteen-Core E5-2698v3. The programs are part of the PARSEC Benchmark Suite \cite{Southern2016} e \cite{Bienia2010} (Princeton Repository for Shared- Memory Computers), which consists of a package with a variety of multi-thread programs, whether OPENMP or Pthreads. Its use is essential, once it represents real computational challenges and, thus, renowned benchmarks. More information about PARSEC may be found in [parsec info].

Despite the promising results at first sight, we have to find a manner to measure how intrusive is the tool, that means, examine how the action of SPERF disturbs the target application to find out, in the last instance, if it happens as to become impracticable.

The tests performed to show the intrusiveness is minimal was to proof that there is no significant distance between the times obtained with SPERF and without SPERF using the one way analysis of variance ANOVA F test \cite{Tiku1971}.This technique can be used to compare means of two or more samples (using the F distribution), testing the null hypothesis that samples in all groups are drawn from populations with the same mean values.

\subsection{BlackSchoels test}
The Blackscholes application is a partial differential equation that, in the way implemented on PARSEC, calculates prices for a portfolio of European options analytically. In our test to this paper, the application was compiled to execute using Pthreads. All the annotations were made on the Blackscholes.c source code where was manually instrumented to perform the tests adding the sperf\_start and sperf\_stop functions available on the sperof.h file on the region of interest. The results shown in the figure \ref{fig:black100b} was perform using the following configuration on SPERF and invoking $./sperf blackscholes$ and the figure \ref{fig:black2100b} using the Linux time program time [13].
\\
\\
$number\_of\_tests=96$\\
$max\_number\_threads=32$\\
$type\_of\_step=power$\\
$value\_of\_step=2$\\
$list\_of\_args=\{in\_312K.txt \quad outputFile.txt\}$

\begin{figure}[H]
	\includegraphics[width=7cm,height=6cm]{figures/black100b}
	\caption{With SPERF}
	\label{fig:black100b}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=7cm,height=6cm]{figures/black2100b}
  \caption{With Linux Time}
  \label{fig:black2100b}
\end{figure}

\subsection{Freqmine test}
Freqmine is an application that employs an array-based version of the FP-growth (Frequent Pattern-growth) method for Frequent Itemset Mining.This program provide multiples alternatives of parallelization and for this test the application was compiled with to execute with OpenMP what makes it possible to use the automatic instrumentation on this application. With the command, ./sperf -i -p /pathToFreqmineFolder, the code was marked on 7 different regions, after that was compiled normally and invoking $./sperf freqmine$ with the configuration on SPERF was obtained the results on the figure \ref{fig:freq100b} and the figure \ref{fig:freq2100b} with Linux time.
\\
\\
$number\_of\_tests=96$\\
$max\_number\_threads=32$\\
$type\_of\_step=power$\\
$value\_of\_step=2$\\
$list\_of\_args=\{kosarak\_990k.dat \quad 1200 \quad output.txt\}$

\begin{figure}[H]
	\includegraphics[width=7cm,height=6cm]{figures/freq100b}
	\caption{With SPERF}
	\label{fig:freq100b}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=7cm,height=6cm]{figures/freq2100b}
  \caption{With Linux Time}
  \label{fig:freq2100b}
\end{figure}


\subsection{Intrusiveness measurement}

The statistical result obtained from the test above from Blackscholes and Freqmine was shown in the tables \ref{tab:blackstatistics} and \ref{tab:freqstatistics}.Where we can compare the result mean and standard deviation from two groups one with SPERF and the other without.

\begin{table}[H]
\centering
\caption{Statistics for BlackSchoels}
\label{tab:blackstatistics}
\begin{tabular}{llll}
                & With Sperf & Without Sperf & Total    \\
N               & 96         & 96            & 192      \\
$\sum{X}$       & 100,19     & 103,0134      & 203,2034 \\
Mean            & 1,0436     & 1,0731        & 1,0584   \\
$\sum{X^2}$     & 105,5093   & 112,4857      & 217995   \\
Std.Dev.        & 0,0998     & 0,1431        & 124     
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Statistics for Freqmine}
\label{tab:freqstatistics}
\begin{tabular}{llll}
            & With Sperf & Without Sperf & Total    \\
N           & 96         & 96            & 192      \\
$\sum{X}$   & 202,8016   & 205,44        & 408,2416 \\
Mean        & 2,1125     & 2,14          & 2,1263   \\
$\sum{X^2}$ & 429,9198   & 440,428       & 870,3478 \\
Std.Dev.    & 0,1256     & 0,091         & 0,1102  
\end{tabular}
\end{table}

Given the summary statistics, the calculations of the hypothesis test are shown in tabular form in the tables \ref{tab:blackAnova} and \ref{tab:freqsAnova}.

\begin{table}[H]
\centering
\caption{Anova f test}
\label{tab:blackAnova}
\begin{tabular}{lllll}
Source             & SS     & df  & MS     &             \\
Between-treatments & 0,0415 & 1   & 0,0415 & F = 2,72671 \\
Within-treatments  & 2893   & 190 & 0,0152 &             \\
Total              & 2,9345 & 191 &        &            
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Anova f test}
\label{tab:freqsAnova}
\begin{tabular}{lllll}
Source             & SS     & df  & MS     &             \\
Between-treatments & 0,0363 & 1   & 0,0363 & F = 3.01548 \\
Within-treatments  & 2,2844 & 190 & 0,012  &             \\
Total              & 2,3206 & 191 &        &            
\end{tabular}
\end{table}

If we take a significance level of 5\% the critical value of F with 1 and 190 degrees of freedom on numerator and denominator respectively is 3.8909. Size booth values of F calculated is lower then the critical there is no significant statistical difference on the mean value according to ANOVA.

%\begin{figure}[H]
%	\includegraphics[width=7cm,height=4.5cm]{ftest}
%	\caption{F distribution}
%	\label{fig:Fdist}
%\end{figure}


\section{Results}
\label{sec:results}

To show how we can interpret the result from SPERF. Was made some analyzes on Freqmine and Blackschoes. The test performed this time was from 1 to 32 threads in a  geometric progression of ratio 2 also varying the data size. The input data used was provide by parsec as a native input and was appropriately divide into several parts such the data size was increase in the same ratio as the number of threads so we can see the scalability.

\subsection{Blackschoes}
The test was performed 4 time and the configuration to this test on Blackscholes was:
\\
\\
$number\_of\_tests=4$\\
$list\_threads\_values=\{\}$\\
$max\_number\_threads=32$\\
$type\_of\_step=power$\\
$value\_of\_step=2$\\
$list\_of\_args=\{$\\
$\texttt{number\_of\_threads} \quad in\_312K.txt \quad out\_312K.txt,$\\
$\texttt{number\_of\_threads} \quad in\_625K.txt \quad out\_625K.txt,$\\
$\texttt{number\_of\_threads} \quad in\_1M.txt \quad out\_1M.txt,$\\
$\texttt{number\_of\_threads} \quad in\_2M.txt \quad out\_2M.txt,$\\
$\texttt{number\_of\_threads} \quad in\_5M.txt \quad out\_5M.txt,$\\
$\texttt{number\_of\_threads} \quad in\_10M.txt \quad out\_10M.txt,$\\
$\}$

There was generated 2 files from Blacksholes one from the entire and other to the region marked size it only have one parallel region. Inside this files was the tables with the efficiency from each configuration. To the a more reliable result was get the median value of the executions and the result was plotted on a 3D graph to a better visualization as shown in the figure \ref{fig:blackRegions}.


\begin{figure}[H]
     \centering
     \subfloat[][Entire application]{\includegraphics[width=6cm,height=5cm]{blackFig1}\label{fig:blackFig1}}
     
     \subfloat[][Region 1]{\includegraphics[width=6cm,height=5cm]{blackFig2}\label{fig:blackFig2}}
     
     \caption{Regions of blackschoels}
     \label{fig:blackRegions}
\end{figure}

From this results we can see that the region that was parallelized has some part there is strong scalable on the argument 2 from 2 cores to 8.
And some parts the efficiency decay and start increases again. This observation was not possible with only the analyses of the entire application. Also we can see in general that is scalable because looking only the direction of the problem size the efficiency was roughly constant.

\subsection{Freqmine}
In the freqmine test size there two parameters that control the problem size was found a combination that represent the data size increases to be roughly the same ratio as the number of thread. The configuration used was:
\\
\\
$number\_of\_tests=4$ \\
$list\_threads\_values=\{\}$ \\
$max\_number\_threads=32$ \\
$type\_of\_step=power$ \\
$value\_of\_step=2$ \\
$list\_of\_args=\{$ \\
$kosarak\_30k.dat \quad 50 \quad kosarak\_30kout.dat,$\\
$kosarak\_61k.dat \quad 60 \quad kosarak\_61kout.dat,$\\
$kosarak\_123k.dat \quad 120 \quad kosarak\_123kout.dat,$\\
$kosarak\_247k.dat \quad 240 \quad kosarak\_247kout.dat,$\\
$kosarak\_495k.dat \quad 540 \quad kosarak\_495kout.dat,$\\
$kosarak\_990k.dat \quad 1400 \quad kosarak\_990kout.dat,$\\
$\}$

This test created eight files.Seven from region marked and one from the entire application. Also plotting the 3D graphs where we can see on figure \ref{fig:freqRegions}.

\begin{figure}[H]
     \centering
     \subfloat[][Entire application]{\includegraphics[width=4cm,height=3.5cm]{freqFig1}\label{fig:freqFig1}}
     \subfloat[][Region 1]{\includegraphics[width=4cm,height=3.5cm]{freqFig2}\label{fig:freqFig2}}
     
     \subfloat[][Region 2]{\includegraphics[width=4cm,height=3.5cm]{freqFig3}\label{fig:freqFig3}}
     \subfloat[][Region 3]{\includegraphics[width=4cm,height=3.5cm]{freqFig4}\label{fig:freqFig4}}
\end{figure}
 
\begin{figure}[H]
     \centering
     \ContinuedFloat
     \subfloat[][Region 4]{\includegraphics[width=4cm,height=3.5cm]{freqFig5}\label{fig:freqFig5}}
     \subfloat[][Region 5]{\includegraphics[width=4cm,height=3.5cm]{freqFig6}\label{fig:freqFig6}}
     
     \subfloat[][Region 6]{\includegraphics[width=4cm,height=3.5cm]{freqFig7}\label{fig:freqFig7}}
     \subfloat[][Region 7]{\includegraphics[width=4cm,height=3.5cm]{freqFig8}\label{fig:freqFig8}}
     
     \caption{Regions of freqmine}
     \label{fig:freqRegions}
\end{figure}


On this application we can see that the regions have complete different profiles. The regions 4 and 7 the efficiency decay drastically with increasing in the number of threads and there is no weak or strong parts. The regions 2, 3 and 5 responds better to the parallelization and we can see that by increase the problem size the efficiency grows significantly and there is a appearance of strong and weak parts. The regions 1 and 6 have some interesting results where we can see efficiency greater than 1 witch implies on super linear speedup. And the overall application have some decays on the efficiency but grew back and in general is scalable.

One of the roles of SPERF gets clearer when examining these figures. Such efficiency profiling may give to the user a deeper idea of how the target application behaves and improve the parallelization on the crucial parts.All of that work to achieve that results on the tables could be done manually, but with a higher cost to the user. The library functions make the code instrumentation simpler and the automatization factor presents and handle the output faster.

\section{Future Work}
\label{sec:future_work}

The tool is still in its first versions, so there are some features that are not supported yet, but, future works should improve these faults. Follow some new features suggestions:
\begin{itemize}
\item Support operating systems other than Linux. 
\item Automatic instrument code using Pthread due to the high granularity presents on this model.
\item Supporting multiples ways of passing program inputs
\item Instrumentation support through the compiler.
\item Support to distributed system MPI(Message Passing Interface).
\item Graphical Interface of better controls and visualizations features.
\end{itemize}


\section{Conclusions}
\label{sec:conclusions}

Scalability of the application is one of the most important ways to measure the limits of a parallel application, and we face a lack of tools that automatize this process. In this sense, this paper proposes SPERF as an alternative to accomplish this task. Considering the results obtained, we suggest that using our profiler may be very convenient.

SPERF makes really simple to analyze the scalability of the program with only a few options to configure and without making extensive modifications to source code it's possible to run the program with different data sizes and cores, and show the user in a presentable way the efficiency of each parallel region on the code.Furthermore, the level of intrusion measured shows that the influence of the instrumentation inside the user's code is minimal, thus confirming the convenience of the profiler proposed.

The future goals of the tool it is to support distributed systems (MPI) and developing a graphical user interface to a better interaction and data presentation.


% Bibliography

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
